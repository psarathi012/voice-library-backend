model_id,author,downloads,likes,tags,pipeline_tag,description,model_type,last_modified,readme
deepseek-ai/deepseek-llm-7b-base,deepseek-ai,40308,76,"transformers, pytorch, llama, text-generation, license:other, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us",text-generation,,,2023-11-30 03:06:57,"--- license: other license_name: deepseek license_link: LICENSE ---  <p align=""center""> <img width=""500px"" alt=""DeepSeek Chat"" src=""https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/images/logo.png?raw=true""> </p> <p align=""center""><a href=""https://www.deepseek.com/"">[üè†Homepage]</a>  |  <a href=""https://chat.deepseek.com/"">[ü§ñ Chat with DeepSeek LLM]</a>  |  <a href=""https://discord.gg/Tc7c45Zzu5"">[Discord]</a>  |  <a href=""https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/images/qr.jpeg"">[Wechat(ÂæÆ‰ø°)]</a> </p> <hr>     ### 1. Introduction of Deepseek LLM  Introducing DeepSeek LLM, an advanced language model comprising 7 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In order to foster research, we have made DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat open source for the research community.     ### 2. Model Summary `deepseek-llm-7b-base` is a 7B parameter model with Multi-Head Attention trained on"
meta-llama/Llama-2-7b,meta-llama,0,4239,"facebook, meta, pytorch, llama, llama-2, text-generation, en, arxiv:2307.09288, license:llama2, region:us",text-generation,,,2024-04-17 08:12:44,No README available
meta-llama/Llama-3.1-8B-Instruct,meta-llama,6035070,3611,"transformers, safetensors, llama, text-generation, facebook, meta, pytorch, llama-3, conversational, en, de, fr, it, pt, hi, es, th, arxiv:2204.05149, base_model:meta-llama/Llama-3.1-8B, base_model:finetune:meta-llama/Llama-3.1-8B, license:llama3.1, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us",text-generation,,,2024-09-25 17:00:57,No README available
meta-llama/Llama-3.1-70B-Instruct,meta-llama,384185,787,"transformers, safetensors, llama, text-generation, facebook, meta, pytorch, llama-3, conversational, en, de, fr, it, pt, hi, es, th, arxiv:2204.05149, base_model:meta-llama/Llama-3.1-70B, base_model:finetune:meta-llama/Llama-3.1-70B, license:llama3.1, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us",text-generation,,,2024-12-15 01:55:33,No README available
meta-llama/Llama-3.1-405B-Instruct,meta-llama,70763,568,"transformers, safetensors, llama, text-generation, facebook, meta, pytorch, llama-3, conversational, en, de, fr, it, pt, hi, es, th, arxiv:2204.05149, base_model:meta-llama/Llama-3.1-405B, base_model:finetune:meta-llama/Llama-3.1-405B, license:llama3.1, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us",text-generation,,,2024-09-25 17:02:11,No README available
HKUSTAudio/Llasa-3B,HKUSTAudio,8080,432,"safetensors, llama, Text-to-Speech, text-to-speech, zh, en, arxiv:2502.04128, base_model:meta-llama/Llama-3.2-3B-Instruct, base_model:finetune:meta-llama/Llama-3.2-3B-Instruct, license:cc-by-nc-4.0, region:us",text-to-speech,,,2025-02-07 06:40:48,No README available
hexgrad/Kokoro-82M,hexgrad,478671,3101,"text-to-speech, en, arxiv:2306.07691, arxiv:2203.02395, base_model:yl4579/StyleTTS2-LJSpeech, base_model:finetune:yl4579/StyleTTS2-LJSpeech, doi:10.57967/hf/4329, license:apache-2.0, region:us",text-to-speech,,,2025-02-01 23:54:42,"--- license: apache-2.0 language: - en base_model: - yl4579/StyleTTS2-LJSpeech pipeline_tag: text-to-speech --- **Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.  <audio controls><source src=""https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/samples/HEARME.wav"" type=""audio/wav""></audio>  ‚¨ÜÔ∏è **Kokoro has been upgraded to v1.0!** See [Releases](https://huggingface.co/hexgrad/Kokoro-82M#releases).  ‚ú® You can now [`pip install kokoro`](https://github.com/hexgrad/kokoro)! See [Usage](https://huggingface.co/hexgrad/Kokoro-82M#usage).  - [Releases](#releases) - [Usage](#usage) - [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md) ‚ÜóÔ∏è - [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/m"
google/gemma-2-9b-it,google,572027,657,"transformers, safetensors, gemma2, text-generation, conversational, arxiv:2009.03300, arxiv:1905.07830, arxiv:1911.11641, arxiv:1904.09728, arxiv:1905.10044, arxiv:1907.10641, arxiv:1811.00937, arxiv:1809.02789, arxiv:1911.01547, arxiv:1705.03551, arxiv:2107.03374, arxiv:2108.07732, arxiv:2110.14168, arxiv:2009.11462, arxiv:2101.11718, arxiv:2110.08193, arxiv:1804.09301, arxiv:2109.07958, arxiv:1804.06876, arxiv:2103.03874, arxiv:2304.06364, arxiv:2206.04615, arxiv:2203.09509, base_model:google/gemma-2-9b, base_model:finetune:google/gemma-2-9b, license:gemma, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us",text-generation,,,2024-08-27 19:41:49,No README available
